{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import word2vec as w2v\n",
    "import gensim\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # must be downloaded beforehand with nltk.download()\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'test how thi thing split whitespac'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "stripRegex = re.compile('[^a-zA-Z]') # strip non-alphanumerics\n",
    "def clean_token(w):\n",
    "    try:\n",
    "        w = porter.stem(''.join([a.lower() for a in w if a.isalnum()]))\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        raise\n",
    "    except:\n",
    "        # problem file (#1068): \"In re Pittsburgh Corning Corp.-Wed Dec 20 16:00:00 PST 2006\"\n",
    "        # problem phrase: \"http:// dictionary. oed. com.\"\n",
    "        # problem token: \"oed.\"\n",
    "        # in cases where porter dies due to interpreting tokens like this as suffixes with missing starts, pass the token through after manual stripping\n",
    "        w = stripRegex.sub('', w)\n",
    "    return w\n",
    "    \n",
    "def clean_phrase(s):\n",
    "    s = s.split()\n",
    "    s = ' '.join([clean_token(w) for w in s])\n",
    "    return s\n",
    "\n",
    "clean_phrase(\"Testing how\\nthis thing splits\\twhitespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concat court cases to train word2vec\n",
    "casepath = 'CourtCases'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0 files\n",
      "Read 1000 files\n",
      "Read 2000 files\n",
      "Read 3000 files\n",
      "Read 4000 files\n",
      "Read 5000 files\n",
      "Read 6000 files\n",
      "Read 7000 files\n",
      "Read 8000 files\n",
      "Read 9000 files\n",
      "Read 10000 files\n",
      "Read 11000 files\n",
      "Read 12000 files\n",
      "Read 13000 files\n",
      "Read 14000 files\n",
      "Read 15000 files\n",
      "Read 16000 files\n",
      "Read 17000 files\n",
      "Read 18000 files\n",
      "Read 19000 files\n",
      "Read 20000 files\n",
      "Read 21000 files\n",
      "Read 22000 files\n",
      "Read 23000 files\n",
      "Read 24000 files\n",
      "Read 25000 files\n",
      "Read 26000 files\n",
      "Read 27000 files\n",
      "Read 28000 files\n",
      "Read 29000 files\n",
      "Read 30000 files\n",
      "Read 31000 files\n",
      "Read 32000 files\n",
      "Read 33000 files\n",
      "Read 34000 files\n",
      "Read 35000 files\n",
      "Read 36000 files\n",
      "Read 37000 files\n",
      "Read 38000 files\n",
      "Read 39000 files\n",
      "Read 40000 files\n",
      "Read 41000 files\n",
      "Read 42000 files\n",
      "Read 43000 files\n",
      "Read 44000 files\n",
      "Read 45000 files\n",
      "Read 46000 files\n",
      "Read 47000 files\n",
      "Read 48000 files\n",
      "Read 49000 files\n",
      "Read 50000 files\n",
      "Read 51000 files\n",
      "Read 52000 files\n",
      "Read 53000 files\n",
      "Read 54000 files\n",
      "Read 55000 files\n",
      "Read 56000 files\n",
      "Read 57000 files\n",
      "Read 58000 files\n",
      "Read 59000 files\n",
      "Read 60000 files\n",
      "Read 61000 files\n",
      "Read 62000 files\n",
      "Read 63000 files\n",
      "Read 64000 files\n",
      "Read 65000 files\n",
      "Read 66000 files\n",
      "Read 67000 files\n",
      "Read 68000 files\n",
      "Read 69000 files\n",
      "Read 70000 files\n",
      "Read 71000 files\n",
      "Read 72000 files\n",
      "Read 73000 files\n",
      "Read 74000 files\n",
      "Read 75000 files\n",
      "Read 76000 files\n",
      "Read 77000 files\n",
      "Read 78000 files\n",
      "Read 79000 files\n",
      "Read 80000 files\n",
      "Read 81000 files\n",
      "Read 82000 files\n",
      "Read 83000 files\n",
      "Read 84000 files\n",
      "Read 85000 files\n",
      "Read 86000 files\n",
      "Read 87000 files\n",
      "Read 88000 files\n"
     ]
    }
   ],
   "source": [
    "#HEAVY\n",
    "# Build clean file for Word2Vec training\n",
    "fns = [fn for fn in os.listdir(casepath) if fn[0] != '.']\n",
    "with open('clean_cases.txt', 'w') as catfile:\n",
    "    for i, fn in enumerate(fns):\n",
    "        with open(os.path.join(casepath, fn)) as infile:\n",
    "            s = clean_phrase(infile.read())+'\\n'\n",
    "            catfile.write(s)\n",
    "        if i % 1000 == 0:\n",
    "            print \"Read %d files\" % i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HEAVY\n",
    "model = w2v.word2vec('clean_cases.txt', 'cases_w2v.bin', size=150, window=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('cases_w2v.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'm', 0.7145407795906067),\n",
       " (u'jr', 0.7127110958099365),\n",
       " (u'h', 0.6810263395309448),\n",
       " (u'e', 0.6772070527076721),\n",
       " (u'jame', 0.6240376830101013),\n",
       " (u'j', 0.6222593188285828),\n",
       " (u'david', 0.6136133670806885),\n",
       " (u'robert', 0.6124848127365112),\n",
       " (u'john', 0.6003983616828918),\n",
       " (u'thoma', 0.5987942218780518)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive='law', negative=['justi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/data/train lines: 300\n",
      "output/test lines: 300\n"
     ]
    }
   ],
   "source": [
    "with open('inputData.csv', 'r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    data = [row for row in reader]\n",
    "\n",
    "with open('outputData.csv', 'r') as testfile:\n",
    "    reader = csv.reader(testfile)\n",
    "    test = [row for row in reader]\n",
    "\n",
    "questions = [d[0] for d in data]\n",
    "answers = [d[1:] for d in data]\n",
    "\n",
    "validation = [t[1] for t in test]\n",
    "\n",
    "print \"input/data/train lines: %d\" % len(data)\n",
    "print \"output/test lines: %d\" % len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Similarity between sentences\n",
    "\n",
    "pre_clean_enabled = True #causes about a 4% gain in accuracy\n",
    "stopwords_enabled = False #causes about a 12% hit to accuracy\n",
    "topn_enabled = True #causes about a 5% gain in accuracy\n",
    "\n",
    "if stopwords_enabled:\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stemmedstop = set([])\n",
    "    for word in stop:\n",
    "        stemmedstop.add(clean_token(word))\n",
    "\n",
    "def similarity(q, a, n=200):\n",
    "        score = 0.0\n",
    "        no_count = 0\n",
    "        \n",
    "        if pre_clean_enabled:\n",
    "            q = clean_phrase(q)\n",
    "            a = clean_phrase(a)\n",
    "        \n",
    "        if stopwords_enabled:\n",
    "            qlist = [i for i in q.split() if i not in stemmedstop]\n",
    "            alist = [i for i in a.split() if i not in stemmedstop]\n",
    "        else:\n",
    "            qlist = q.split()\n",
    "            alist = a.split()\n",
    "        \n",
    "        for wq in qlist:\n",
    "            for wa in alist:\n",
    "                try: # Shouldn't happen anymore\n",
    "                    score += model.similarity(wq, wa)\n",
    "                except: # Shouldn't happen anymore\n",
    "                    no_count += 1\n",
    "        score /= (len(q)*len(a) - no_count)\n",
    "        return score\n",
    "\n",
    "def similarity_new(q, a, n=0.4):\n",
    "        scores = []\n",
    "        no_count = 0\n",
    "        \n",
    "        if pre_clean_enabled:\n",
    "            q = clean_phrase(q)\n",
    "            a = clean_phrase(a)\n",
    "        \n",
    "        if stopwords_enabled:\n",
    "            qlist = [i for i in q.split() if i not in stemmedstop]\n",
    "            alist = [i for i in a.split() if i not in stemmedstop]\n",
    "        else:\n",
    "            qlist = q.split()\n",
    "            alist = a.split()\n",
    "        \n",
    "        for wq in qlist:\n",
    "            for wa in alist:\n",
    "                try: # Shouldn't happen anymore\n",
    "                    scores.append(model.similarity(wq, wa))\n",
    "                except: # Shouldn't happen anymore\n",
    "                    pass\n",
    "        \n",
    "        if topn_enabled:\n",
    "            num = min(int(float(len(scores))*n),len(scores))\n",
    "        else:\n",
    "            num = len(scores)\n",
    "        score = float(sum(scores[-num:])) / float(num)\n",
    "        return score\n",
    "\n",
    "def rank_answers(q, alist):\n",
    "    # Find answer that maximizes inner product of embeddings\n",
    "    # Credit: Miyyer\n",
    "    simlist = []\n",
    "    for a in alist:\n",
    "        simlist.append(similarity_new(q,a))\n",
    "    #return [a for (a, s) in sorted(zip(alist, simlist))]\n",
    "    return alist[simlist.index(max(simlist))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43333333333333335"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#moderately heavy\n",
    "subset = False\n",
    "if subset:\n",
    "    smallquestions = questions[150:200]\n",
    "    smallanswers = answers[150:200]\n",
    "    smallvalidation = validation[150:200]\n",
    "else:\n",
    "    smallquestions = questions\n",
    "    smallanswers = answers\n",
    "    smallvalidation = validation\n",
    "\n",
    "\n",
    "top_answer = [rank_answers(q,a) for q,a in zip(smallquestions,smallanswers)]\n",
    "correct = [int(i == j) for (i,j) in zip(top_answer, smallvalidation)]\n",
    "(sum(correct)*1.0)/len(smallvalidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HEAVY\n",
    "top_answer = [rank_answers(q,a) for q,a in zip(questions,answers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3566666666666667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = [int(i == j) for (i,j) in zip(top_answer, validation)]\n",
    "(sum(correct)*1.0)/len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Section 502(b)(6) caps a landlord\\'s claim in bankruptcy for damages resulting from the termination of a real property lease.16 Under \\xc2\\xa7 502(b)(6), a landlord-creditor is entitled to rent reserved from the greater of (1) one lease year or (2) fifteen percent, not to exceed three years, of the remaining lease term. The cap operates from the earlier of the petition filing date or \"the date on which [the] lessor repossessed or the lessee surrendered, the leased property.\" The landlord also retains a claim for any unpaid rent due under such lease prior to the earlier of those dates. This language reflects Congress\\'s intent to limit lease termination claims to prevent landlords from receiving a windfall over other creditors. See H.R.Rep. No. 95-595, at 353 (1977), reprinted in 1978 U.S.C.C.A.N. 5963, 6309 (\"[The cap] limits the damages allowable to a landlord of the debtor.... It is designed to compensate the landlord for his loss while not permitting a claim so large (based on a long-term lease) as to prevent other general unsecured creditors from recovering a dividend from the estate. The damages a landlord may assert from termination of a lease are limited....\"); 4 Collier on Bankruptcy, \\xc2\\xa7 502.03 at 7a (\"[The cap is] designed to compensate the landlord for his loss while not permitting a claim so large as to prevent other general unsecured creditors from recovering a dividend from the estate.\")'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_answers(questions[0],answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
